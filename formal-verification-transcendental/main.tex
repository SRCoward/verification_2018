\NeedsTeXFormat{LaTeX2e}
\documentclass{fac}
\ifprodtf \else \usepackage{latexsym}\fi
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{url,booktabs}
\usepackage{caption}
\usepackage{subcaption}

\usepackage{bm}
\usepackage{amssymb,amsmath}
\usepackage{topcapt,booktabs}
\usepackage{tikz}
\usepackage{pgfplots} 
\usepackage{listings}
\usepackage{appendix}
\pgfplotsset{compat=newest}
\usetikzlibrary{shapes,arrows,positioning}
\usetikzlibrary{shapes.arrows,patterns}

\newcommand{\abs}[1]{\lvert#1\rvert}
\newcommand{\floor}{\mathop{\textrm{floor}}\nolimits}

\title[Transcendental Verification using Theorem Proving]{Formal Verification of Transcendental Fixed and Floating Point Algorithms using an Automatic Theorem Prover}


\author[Samuel Coward]
    {Samuel Coward$^1$, Lawrence Paulson$^2$, Theo Drane$^3$ and Emiliano Morini$^3$\\
     $^1$Faculty of Mathematics, University of Cambridge,\\
     $^2$Computer Laboratory, University of Cambridge,\\
     $^3$Cadence Design Systems, Cambridge\\}

\correspond{Samuel Coward, 58 Grove Hall Court, 2 Hall Road, St Johns Wood, London, NW8 9NY, UK.\\
            e-mail: s.coward111@gmail.com}

\pubyear{2020}
\pagerange{\pageref{firstpage}--\pageref{lastpage}}

\begin{document}
\label{firstpage}

\makecorrespond

\maketitle

\begin{abstract}
We present a method for formal verification of transcendental hardware and software algorithms that scales to higher precision without suffering an exponential growth in runtimes. A class of implementations using piecewise polynomial approximation to compute the result is verified using MetiTarski, an automated theorem prover, which verifies a range of inputs for each call. The method was applied to commercial implementations from Cadence Design Systems with significant runtime gains over exhaustive testing methods and was successful in proving that the expected accuracy of one implementation was overly optimistic. Reproducing the verification of a sine implementation in software, previously done using an alternative theorem proving technique, demonstrates that the MetiTarski approach is a viable competitor. Verification of a 52 bit implementation of the square root function highlights the methods high precision capabilities.  
\end{abstract}
\begin{keywords}Theorem prover; Transcendental functions; Floating point algorithms; Hardware implementation
\end{keywords}

\section*{Declarations}
\textbf{Funding:} The conceptualisation and much of the preliminary work for this paper was done during a summer internship at Cadence Design Systems.
The development of MetiTarski was supported by EPSRC grants EP/C013409/1 and EP/I011005/1.

\noindent\textbf{Conflicts of interest:} Theo Drane and Emiliano Morini have both now moved on to new roles and are no longer affiliated with Cadence Design Systems. 

\noindent\textbf{Availability of data and material:} All problem scripts, excluding the examples verifying commercial Cadence Design Systems' implementations, can be found on GitHub \newline (https://github.com/SRCoward/verification\_2018.git).

\noindent\textbf{Code availability:} MetiTarski is an open source theorem prover. It can be downloaded from
\newline \url{https://www.cl.cam.ac.uk/~lp15/papers/Arith/}. It must be paired with an additional solver, for which there are several options documented in the MetiTarski user guide.

\noindent\textbf{Authors' contributions:} The study conception and design was led by Theo Drane and Emiliano Morini. Material preparation and analysis were performed by Samuel Coward, Lawrence Paulson and Emiliano Morini. The first draft of the manuscript was written by Samuel Coward and edited by Lawrence Paulson.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%INTRODUCTION
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
\label{intro}
Formal verification of floating point operations is becoming ever more challenging as hardware designs reach levels of complexity only previously seen in software. Its importance in industry is well known, exemplified by the Pentium floating point division bug \cite{pratt1995anatomy}. The bulk of this research was conducted during a project at Cadence Design Systems. Building upon this, we present a new approach to the verification of fixed and floating point transcendental algorithms. This technique should be viable for verifying high precision algorithms, as our findings suggest that runtimes will not rise exponentially with the precision.  Our experiments cover implementations of logarithms, square root and the sine function; however, the methodology can also be applied to many different functions implemented using algorithms of the form described in \S \ref{trans functs}. Verification of logarithm implementations is a relevant problem as it finds applications in fields such as digital signal processing and 3D graphics \cite{lewis1995114,harris2001powering}. In addition, it can be very simple to implement, with one of the simplest examples of a floating point logarithm using the exponent as the integer part of the result combined with a lookup table (LUT) for the most significant bits (msb) of the significand, to generate the fractional part \cite{harris2001powering}.
\begin{equation*}
    \log(2^{\textit{exp}} \times 1.sig) \approx \textit{exp} + LUT(\textit{sig}[msb])
\end{equation*}

% Traditional verification techniques
Traditional techniques rely on exhaustive testing of all inputs to verify such algorithms, but this can be resource intensive, perhaps prohibitively so. The Multiple Precision Floating-Point Reliable (MPFR) library is a C library for multiple-precision floating point computations with correct rounding \cite{fousse2007mpfr}, and is widely used as a reference for many verification tasks. For example, some of the industrial implementations presented here were verified by comparing the outputs to the MPFR library. We shall see that the methodology used in this paper performs more efficiently in particular cases. 

% Functions to verify
The paper will focus on implementations of transcendental functions in hardware that rely on piecewise polynomial approximations. Many elementary functions are traditionally calculated in software \cite{10.5555/1096483,gal1991accurate}, but for numerically intensive algorithms such implementations may simply be too slow, leading to the development of dedicated hardware \cite{tang1991table,strollo2011elementary,pineiro2004algorithm}. Although primarily focusing on hardware, some software implementations may be amenable to the verification approach presented here, as we shall see in \S\ref{gappa}. De Dinechin, Lauter and Melquiond verified a CRlibm binary64 library function using the Gappa proof assistant \cite{daramy2009cr,5483294}. Their approach is potentially the most similar method (in execution) to ours and therefore we will also use our method to verify the same function. All the examples considered here use a binary representation, but the simplest decimal floating point implementations, that convert to binary, use the binary algorithm, then convert back to decimal would also be amenable \cite{5223332}. Other decimal floating point implementations appear to rely more on the digit-recurrence algorithm \cite{5223326,5710893}, which is more challenging to reduce to a series of inequalities since decisions are typically made at each iteration based on information from the previous iterations.

% Commented out as not sure it is the right place
%In floating point arithmetic, the upper most portion of the input significand is passed to a lookup table which outputs a number of coefficients. The remainder of the input is then used to compute a polynomial approximation to %the exact result. Scaling to handle the exponent is often computed in a separate section of the code. Such an approach is easy to verify using the methodology described in this paper, but will hold for any implementation %where it is possible to abstract an underlying mathematical model that can be expressed as a system of inequalities for verification purposes.

% MetiTarski Introduction
To produce the required proofs we use MetiTarski~\cite{akbarpour2010metitarski}, an automatic theorem prover for real valued analytic functions, such as cosine and logarithm. It's a combination of a resolution theorem prover and a decision procedure for the theory of real closed fields (RCF). A field is real closed if every positive number has a square root, which is equivalent to saying that it has the same first-order properties as the reals. The resolution prover at the base of MetiTarski is Joe Hurd's Metis \cite{metis_theorem_prover}, which is modified in several respects \cite{akbarpour2008metitarski}. One key modification is to the ordering of the resolution prover \cite{ludwig2007extension}, which encourages the replacement of supported functions by bounds. The inbuilt axioms are primarily upper and lower bounds on a set of supported functions. The choice of these bounds was carefully considered. Many are based on the bounds proposed by Daumas, Lester and Munoz \cite{daumas2008verified}; these are typically derived from Taylor series. Other bounds are obtained from continued fraction expansions, for example
\[\frac{1}{2} \leq x \leq 1 \implies \frac{x-1}{x} \leq \ln(x) \leq \frac{3x^2-4x+1}{2x^2}.\]

The resolution prover applies the axioms, replacing any supported funtions and generates polynomial inequalites. With the problem reduced to the theory of RCFs, the decision procedure is then sufficient to finalise the proof. Conjectures are passed to MetiTarski as a set of inequalities which are transformed by replacing any special function by an appropriate bound. Typically proofs are found in a few seconds \cite{akbarpour2009applications}, but if MetiTarski is unable to prove a conjecture it does not mean that the conjecture is false. For verification it is important that MetiTarski produces machine readable proofs that include algebraic simplification, decision procedure calls, and resolution rules \cite{denman2009formal}. MetiTarski is among a limited number of automated tools supporting transcendental functions. Examples include the first-order logic automated reasoning tool \texttt{dReal} \cite{gao2013dreal} and an approach to Satisfiability Modulo the theory of transcendental functions, which used properties from the MetiTarski suite as benchmarks \cite{cimatti2017satisfiability}. Our initial research encouraged the MetiTarski developers to add an axiom to bound the floor function. The main benefit of the update, for this paper, is to simplify the syntax and construction of our conjectures.  

To give an early outline of the approach, we take some piecewise polynomial implementation, that uses polynomials $p_i(x)$ for $i=0,1...,K$ and $x \in \Sigma_i$, to approximate a function $f(x)$, for $x\in \Sigma$, where $I$ is some input domain. The piecewise polynomial implementation is generally accompanied by some claimed error bound, $\epsilon$, which is what we aim to prove. More precisely, we prove that
\begin{equation*}
    \forall i=0,1,...,K \textrm{ and } \forall x\in \Sigma_i, \, \abs{f(x)-p_i(x)}<\epsilon.
\end{equation*}
For each $i$, we will generate at least one problem to be automatically proven by MetiTarski. The main novelty of this contribution is in the application of MetiTarski to this verification problem. MetiTarski's understanding of elementary functions means that it is the only necessary theorem proving tool required in this methodology, unlike other approaches that have combined tools \cite{5483294}. The automatic generation of problem files from a template problem makes the verification effort simpler and could be used with other automatic theorem provers.

% Summary of paper sections
In \S \ref{trans functs} we will discuss the common approaches to implementations of elementary functions. In \S \ref{Method} we will describe the verification methodology of this paper, with actual applications and results of using this method presented in \S \ref{Apps} and \S \ref{flopoco}. Lastly, we will compare our approach to other theorem proving verification methods in \S \ref{gappa} and \S \ref{Related}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Transcendental Function Implementations
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Transcendental Function Implementations}
\label{trans functs}

% General methods
As mentioned above, transcendental functions are commonly implemented in software, however the numerical algorithms used in software are often unsuitable for hardware. One example, using Chebyshev series expansion, would result in a high area circuit due to its use of a variety of expensive operations such as floating point multiplication \cite{fowkes1993hardware}. There are lots of different hardware algorithms to implement these functions, but a large proportion fall into one of the following categories: digit-recurrence \cite{bajard1994bkm,1030708}, CORDIC  (COordinate Rotation DIgital Computer) \cite{volder1959cordic,andraka1998survey,10.1145/1478786.1478840} or table-lookup \cite{tang1991table,story1999new}. A comparison of the different algorithms is beyond the scope of this paper but has been tackled by other authors \cite{tang1991table,pineiro2004algorithm}. We will focus on table-lookup algorithms, as they are broadly used and are the most amenable to our methodology .

%Table lookup outline
Tang's 1991 paper provided a general framework for implementing a function $f$ on an interval $I$, which most table driven algorithms use to some degree \cite{tang1991table}. According to Tang, typical table-lookup algorithms have a set of \textit{breakpoints} $c_k \in I$ for $k=1, 2, ... N$, along with a table of approximations $T_k$, such that $f(c_k) \approx T_k$. Given $x \in I$, the algorithm uses the following steps to calculate $f(x)$:
%Evaluation of steps to calculate a table-lookup algorithm.
\begin{enumerate}

\item \textbf{Reduction}: Solve $k = min_k \abs{x-c_k}$, then apply a \textit{reduction transformation} $r = R(x,c_k)$.

\item \textbf{Approximation}: Calculate $f(r)$ using an approximating function $p(r)$, often a polynomial is used here.

\item \textbf{Reconstruction}: Using a reconstruction function $S$, which is determined by $f$ and $R$, a final approximation is found.
\begin{align*}
    f(x) &= S(f(c_k),f(r)) \\
         &\approx S(T_k,p(r)).
\end{align*}

\end{enumerate} 

%Sizes of lookup tables and industrial apps
In the rest of the paper, Tang describes algorithms for $2^x$, $\log(x),$ and $\sin{(x)}$, for relatively narrow intervals. In these examples, tables ranging in size from 32 to 64 entries are used. To support wider intervals, further transformations of the arguments to these narrow domains are necessary. 
For example, Tang proposes an algorithm to compute $\ln(x)$ for $x\in[1,2]$, which uses breakpoints $c_k = 1 + k/64$ for $k=0,1,...,64$. The breakpoint is chosen which satisfies $\abs{x-c_k} < 1/128$ and a reduced argument $r=2(x-c_k)/(x+c_k)$ is used to compute a polynomial approximation $p(r)$. The final approximation is given by $\ln(x) \approx T_k + p(r)$, where $T_k \approx \ln(c_k)$ are the tabulated values \cite{tang1991table}.
%For example, a common sine transformation is to subtract an integer multiple, $N$, of $\frac{\pi}{2}$ from the argument, $x$, such that $x-N \times \frac{\pi}{2} \in [-\frac{\pi}{4}, \frac{\pi}{4}]$ \cite{harrison2006floating,harrison2000formal}. 
Polynomials are a common choice for approximating with a reduced argument, and to calculate the coefficients there are a number of approaches. Some use the Remez algorithm to generate the coefficients of the minmax polynomial \cite{veidinger1960numerical,tang1991table}, while others opt to use carefully rounded coefficients from the function's Chebyshev expansion \cite{378099}. For the IA-64 architecture, Intel provided a library of table based algorithms for computing several transcendental functions \cite{harrison1999computation}. The tables used in this library range in size from 24 to 256 double extended entries, for the exponential and logarithm, respectively.

% LUT with piecewise-poly approx
Table based algorithms have been further developed and modified to use lookup tables (LUTs) to construct piecewise polynomial approximations to some elementary functions \cite{strollo2011elementary,pineiro2005high}. The reduction step still uses the breakpoint method, but the table no longer returns just an approximation, $T_k$, it now returns multiple polynomial coefficients for each lookup. Strollo, De Caro and Petra present, alongside their algorithm, many different divisions of the input domain. For example, to compute $\ln(1+x)$ for $x\in[0,1]$ using a piecewise-quadratic approximation, accurate to 24 fractional bits, they required 128 table entries of coefficients \cite{strollo2011elementary}. In the industrial implementation described below, a table containing 32 breakpoints is used. 

To further understand the relationship between LUT size and architecture choices, we can experiment with the FloPoCo tool \cite{DinechinPasca2011-DaT}. FloPoCo can automatically generate piecewise polynomial approximations to any supported function that are accurate to 1 ULP for a given bitwidth. Figure \ref{fig:flopo_graph} shows how the LUT size required changes with the choice of polynomial degree. The second graph highlights how if we use quadratic polynomials and increase the precision of the approximation, exponentially more LUT entries are required. FloPoCo will be looked at in greater detail in \S \ref{flopoco}.

\begin{figure}
\centering
\begin{subfigure}{.45\textwidth}
  \centering
  \begin{tikzpicture}[scale=.75]
    \begin{axis}[
       ymode=log,
       log basis y={2},
       axis lines = left,
       xlabel = {Polynomial degree},
       ylabel = {LUT Entries}]

    \addplot[
       color=blue,
       mark=x,
       ]
       coordinates {(1,2048)(2,64)(3,16)(4,4)(5,2)(6,1)};

    \end{axis}
    \end{tikzpicture}
  \caption{Exponential relationship between polynomial degree and LUT entries.}
  \label{poly_deg_vs_lut}
\end{subfigure}\hspace{1em}%
\begin{subfigure}{.45\textwidth}
  \centering
  \begin{tikzpicture}[scale=.75]
    \begin{axis}[
       ymode=log,
       log basis y={2},
       axis lines = left,
       xlabel = {Input bitwidth},
       ylabel = {LUT Entries},
       xtick = {8, 16, 24, 32, 40},]

    \addplot[
       color=blue,
       mark=x,
       ]
       coordinates {(8,  2)(16, 16)(24, 128)(32, 1024)(40, 4096)};

    \end{axis}
    \end{tikzpicture}
  
  \caption{Using quadratic interpolation for increasing input bitwidths.}
  \label{input_width_vs_lut}
\end{subfigure}
\caption{FloPoCo \cite{DinechinPasca2011-DaT} generated piecewise polynomial approximations to $e^{x}$, for $x \in [0,1]$ and a 1 ULP error bound.}
\label{fig:flopo_graph}
\end{figure}

In this paper, the input to the function will often be represented in floating point format \cite{goldberg1991every}, which stores a number as $(-1)^{s} \times 2^{e-b} \times 1.\textit{significand}$. In IEEE-754 standard single precision, $s$ is a single bit representing the sign, the exponent $e$ is an 8 bit integer, the bias $b$ is a constant equal to 127, and the significand is 23 bits long. The implementations of the logarithm verified in this paper rely on the following identity:
\begin{align*}
    \log(2^{\textit{exp}} \times 1.\textit{significand}) &= \log(2^{\textit{exp}}) + \log(1.\textit{significand}) \\
                                                &= \textit{exp} + \log(1.\textit{significand}).
\end{align*}
The reconstruction step then simply involves adding the integer exponent to an approximation to $\log(1.\textit{significand})$. This approximation passes the top $k$ bits of the significand to a lookup table, which returns coefficients for a degree $m$ polynomial, evaluated using the remaining low bits of the significand. Clearly, if the polynomial is a constant polynomial, then that is equivalent to the $T_k$ described above. As this approach is essentially an enhancement to the method described by Tang, the verification of these piecewise polynomial LUT methods could also be adapted to the simpler LUT methods.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Verification Methodology
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Verification Methodology}
\label{Method}
% Translation into MetiTarski problem
Given an implementation of a transcendental function following the outline above, we obtain an abstraction that is verifiable using MetiTarski. If the top $k$ bits of the significand are passed to a lookup table, for a fixed 8-bit exponent and sign bit, we reduce the verification problem to $2^k$ calls to MetiTarski. Therefore, the  full verification over all inputs is reduced to 
just $2^{k+8+1}$ MetiTarski conjectures to be proven. In some cases, verification over all such inputs is not necessary: a bespoke hand proof may be able to confirm the correctness of the results for exponent scalings. Of course, most verification tasks use massively parallel processing to reduce the runtimes. Similar methods may be used to reduce the runtimes in our approach, as the conjectures passed to MetiTarski are independent of each other. In nearly all commercial implementations, $k$ is relatively small as lookup tables can have high ROM demands \cite{strollo2011elementary}. Assuming that our interpolation coefficients are stored in a file, and we can express the problem as a set of inequalities, the procedure follows the same basic outline. \\

% Method outline
\noindent\textbf{Procedure Outline} (see Figure \ref{procedure_flow})
\begin{enumerate}
\item Write a template problem for the inequalities to be proven, with placeholders for all the relevant interpolation coefficients and upper bit values.
\item Use a wrapper script to read the coefficients and replace them in the template problem to generate the full set of MetiTarski problems.
\item Run the Perl script that accompanies MetiTarski to test all of the problems.
\item Refine error modeling on problems that are not proven and return to step 1.
\item Exhaustively test regions where MetiTarski was unsuccessful.
\end{enumerate}  

% Flow diagram for procedure
\begin{figure} 
\centering
\input{procedure.tex}
\caption{Flow diagram of the verification procedure. Always start by generating an initial template problem from the given implementation.}
\label{procedure_flow}
\end{figure}

% Introduction of toy implementation
To demonstrate the methodology, we analyse a toy implementation for computing the natural logarithm based on the outline above. The implementation takes as an input an 8 bit integer $x_0x_1...x_7$ and outputs an approximation to $\ln(1.x_0...x_7)$. The top four bits, $i = x_0..x_3$, are passed to a lookup table that returns the 10 bit interpolation coefficients $a_i, b_i, c_i$, for $i=0,...,15$. The coefficients are generated using a simple quadratic interpolation scheme. Writing $x=0.x_0...x_7$, the approximation generated is,
\[ \overline{\ln(1+x)}=c_i + b_i x +a_i x^2. \]
This example is designed to be simple to show the underlying principles of the verification methodology. Later, we shall adapt it to be more relevant to industrial implementations. In this case, the implementation is accurate to $2^{-10}$, which can easily be verified using exhaustive testing as the input space only contains $2^8$ values.

% Statement of problem to prove
\[\abs{\ln(1+x)-\overline{\ln(1+x)}} <2^{-10} \qquad (x= 0.x_0...x_7) \]

% Method description
The first step is to generate the template problem. In this problem, the upper bits are represented by a constant, $\_\_y=0.x_0...x_3$, and the lower bits, $X$, are modelled as a MetiTarski variable, to which we assign a specified range. The coefficients, which will be replaced in step 2 of the procedure, are just placeholders, $\_\_a, \_\_b$ and $\_\_c$. If this were a real hardware implementation the design could be improved by absorbing $\_\_y$, a constant, into the pre-calculated coefficients, resulting in a polynomial just in $X$.
% Template Problem
\[ 0\le X \le 2^{-4}-2^{-8} \implies \abs{\ln(1+\_\_y+X)-(\_\_c+\_\_b(\_\_y+X)+\_\_a(\_\_y+X)^2)} < 2^{-10} \]

% Wrapper script and MetiTarski Proofs
A wrapper script now generates the 16 MetiTarski problems, replacing the placeholders $\_\_a, \_\_b$ and $\_\_c$ with the actual coefficients from the LUT and $\_\_y$ with the relevant constant input to the LUT. A Perl script, which is supplied with MetiTarski, automates the calls to our prover, providing a true or false result for each problem. For our toy implementation, MetiTarski is able to provide proofs for all of these problems and therefore steps 4 and 5 of the procedure are rendered redundant. Next we enhance the toy implementation and start to see where the refinement step is useful. The total runtime was 5.3 seconds and no more than 0.4 seconds is spent on any one proof. On such a small input space, exhaustive search is quicker by several orders of magnitude, taking less than a tenth of a second. However, as we shall see, our technique does not suffer from exponentially increasing runtimes as we increase the precision of the implementation. 

% Enhanced Toy Implementation
With this basic understanding of the methodology, we shall make the toy implementation more realistic. Commercial hardware engineers have constraints on area and performance targets to meet, so they apply techniques to reduce the resource requirements. One of these is to truncate bits throughout the algorithm, reducing the number of adders required. This generally improves the performance but typically with some cost to the accuracy of the approximation. In our implementation, we choose appropriate terms to truncate in order to more closely replicate commercial algorithms. The new implementation returns an approximation of the form, 
% Approximation
\begin{equation} \label{update_toy}
\overline{\ln(1+x)}=c + 2^{-8} \lfloor{b(x_0...x_7)} \rfloor +a(0.x_0...x_5)^2. 
\end{equation}
In this case, the approximation is accurate to $2^{-7}$.

% Statement of accuracy
\[ \abs{\ln(1+x)-\overline{\ln(1+x)}} <2^{-7} \qquad (x= 0.x_0...x_7) \]

% Floor function 
This can easily be checked using exhaustive testing, but notice that the implementation uses bit truncation on the first and second order terms. Since MetiTarski has no understanding of the integers, such non-analytic functions are difficult to model. In HOL Light, Harrison developed an entire theory of floating point arithmetic to more closely model the hardware he intended to verify \cite{harrison1999machine}. For our purposes, it was sufficient to explore only simple approximations and bounds to such arithmetic. Inspired by this research, MetiTarski now includes support for a floor function. MetiTarski understands functions via axioms that give upper and lower bounds, and in the case of the floor function we simply have $x-1< \floor(x)\le x$. It should be noted that this bound is poor when the inputs under investigation are close to 1, for example MetiTarski will fail to prove $\floor(0.5)\geq 0$. A simple extension could be the introduction of a bit truncation function, that takes an input $x$ as well as the number of bits to truncate $y$. This would yield bounds:
\begin{equation*}
    x - (1-2^{-y}) \leq \textrm{trunc}(x,y)\leq x
\end{equation*}

However, for the examples investigated in this paper, the basic floor function is sufficient to verify the function to the same precision that can be verified via exhaustive testing. Therefore, using this function, it is possible to produce a MetiTarski conjecture. We now allow $X$ to be the integer value of the bottom 4 bits ($x_4x_5x_6x_7$) and $\_\_y$ the integer value of the top 4 bits ($x_0x_1x_2x_3 0000$), which, as before, determines the coefficients $\_\_a, \_\_b, \_\_c$. We now use the integer values of $X$ and $\_\_y$, because doing so allows us to model bit truncation using the floor function. Our new template problem is then, 
% Enhance toy template problem
\[
0\le X \le 15 \implies \abs{\ln(1+2^{-4}\_\_y+2^{-8}X) - \overline{\ln(1+x)}} <2^{-7} \qquad (x= 0.x_0...x_7)
\]
where,
\[
\overline{\ln(1+x)}=\_\_c+2^{-8}\floor(\_\_b(X+2^{4}\_\_y))+2^{-12}\_\_a(\floor(2^{-2}X+2^{2}\_\_y))^2.
\]

% Error correlation and flaw in its approach.
Using the floor function provides a simple, but usually effective, model. However, this approach has an issue, which it shares with interval arithmetic, in that all correlation of errors is lost by this approach. To demonstrate this problem consider the following equation and MetiTarski floor function model of it, where $z$ is a 4 bit integer.
\[
(z>>1) - (z>>2) \rightarrow  \floor(\frac{1}{2}z) - \floor(\frac{1}{4}z)
\]
\[
\floor(\frac{1}{2}z) \in [\frac{1}{2}z-1, \frac{1}{2}z], \quad \floor(\frac{1}{4}z) \in [\frac{1}{4}z-1,\frac{1}{4}z]
\]

The floor function model indicates that this equation is bounded below by $\frac{1}{2}z - 1 - \frac{1}{4}z = \frac{1}{4}z - 1$. In actual fact the equation is bounded below by $\frac{1}{4}z - \frac{1}{4}$. This discrepancy is a result of disregarding any correlation between the two terms in the model and modelling it as a floor function rather than truncation. This issue also occurs in interval arithmetic, but fortunately we can deploy additional variables in our model which account for some of this correlation. By using two additional error variables our MetiTarski problem can model this behaviour more accurately.
\begin{align*}
(z>>1) - (z>>2) &\rightarrow \frac{1}{2}z - \epsilon_0 - (\frac{1}{4}z - \epsilon_1 - \frac{1}{2}\epsilon_0) \qquad z \in [0,15], \quad \epsilon_0 \in [0,\frac{1}{2}], \quad \epsilon_1 \in [0,\frac{1}{2}] \\   
                &=           \frac{1}{4}z + \epsilon_1 - \frac{1}{2}\epsilon_0 \in [\frac{1}{4}z - \frac{1}{4}, \frac{1}{4}z + \frac{1}{2}]
\end{align*}

%Error Variables
Essentially, the error variables, $\epsilon_0$ and $\epsilon_1$, bound bits 0 and 1 of the variable $z$, allowing us to attain the bound we found analytically above, tighter than the bound we found using the floor function approach. Clearly, if we were to introduce an error variable for every bit truncated, then the number of variables in our MetiTarski problem would grow to be huge for any real world design. This is problematic as MetiTarski runtimes can be doubly exponential in the number of variables, an inherent limitation of the decision procedure on which it relies. In addition, this approach to error modelling requires significantly more user input and skill than the simple floor function method. Intellectual effort is needed to calculate the correlation between error terms introduced in the hardware algorithm. We can also model errors more carefully using a truncation model rather than the floor function approach to improve the tightness of the error bounds. 

%Refinement Step
This example is intended to highlight the limitations of the floor function method, and to demonstrate that with additional human and computational effort it is possible to refine our error models. Returning to the procedure outline given above, the floor function method should be used to generate the initial template problem in step 1. If in step 3, MetiTarski fails to prove all the problems it is given, we can introduce error variables in our template problem to refine our error model. This is typically an iterative approach, as there is a tradeoff between the human effort required and the tightness of the error bounds. So there may be several rounds of error model refinement with MetiTarski runs until either, the problems are all proven or the error model refinements are exhausted. If, after exhausting all error model refinements, some of the MetiTarksi problems remain unproven, then the last step is to use exhaustive testing on the remaining regions. All this effort is typically not wasted, as proving a subset of the MetiTarski problems will reduce, possibly significantly, the size of the input space left for verification using exhaustive testing. In \S \ref{Apps}, we discuss a complex industrial implementation where this final exhaustive step was necessary to complete the verification. This is the only case where we required exhaustive testing.

To illustrate this approach, we model our updated implementation, eqn \ref{update_toy}, using error variables rather than the floor function. This yields a new MetiTarski problem,
\[
	0\le X \le 15 \hspace{0.1cm} \land \hspace{0.1cm} 0\le \epsilon_0<1\hspace{0.1cm} \land \hspace{0.1cm} 0\le \epsilon_1<1
	\implies \abs{\ln(1+2^{-4}\_\_y+2^{-8}X) - M\_\ln(1+x,\epsilon_0,\epsilon_1)} < 2^{-7}
\]
where, 
\[
M\_\ln(1+x,\epsilon_0,\epsilon_1)=\_\_c+2^{-8}(\_\_b(X+2^{4}\_\_y)-\epsilon_0)+2^{-12}\_\_a((2^{-2}X+2^{2}\_\_y)-\epsilon_1)^2.
\]

Surprisingly, for this particular problem, using additional error variables rather than the floor function actually had minimal impact on the overall runtime for the 16 problems. However, the floor function is a recent addition to MetiTarski and we may be able to improve its performance by fine-tuning its heuristic parameters. On industrial implementations, to limit the number of variables it was sometimes necessary to combine error variables and manually calculate an upper bound on these. This was often challenging: some of the error variables can be highly correlated. The floor function method makes the process of generating an initial template problem significantly simpler. For the industrial implementations verified in this paper, error variables were necessary, since tight error bounds were required.
\begin{figure}
\centering
\scalebox{0.9}{\input{runtime.tex}}
\caption{A graph demonstrating the runtime comparison of the competing verification procedures on implementations of growing precision, results obtained running on a single core Intel I7-3517U \label{runtime_graph}}
\end{figure}

% Larger input space
To see why this technique is powerful, we extend the implementation above to larger input spaces. The approximation is essentially the same, using the same coefficients and lookup table, but our input now may be 10 bits rather than 8 bits, for example. Figure \ref{runtime_graph} compares the runtimes of our methodology and exhaustive testing as the input space grows. Notably, the MetiTarski method has roughly constant runtimes, as we expect: MetiTarski is an analytic tool, so increasing the space of discrete inputs only minimally alters the MetiTarski problems. Conversely, exhaustive testing runtimes suffer from exponential growth in the number of bits of the input. Of course, if the size of the lookup table increases, this will affect the MetiTarski runtimes as the number of problems will grow exponentially. The tables used in these algorithms are typically not prohibitively large, those referenced in \S 2 contained less than 256 entries, since large tables translate into additional silicon in hardware designs.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Applications and Discussion
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Applications and Discussion}
\label{Apps}
% Introduction to section
We present some results from applying this methodology to the verification of several larger commercial implementations. The runtime results can be found in Table \ref{result} and demonstrate that the technique has real world relevance.

% Block diagram of implementation
\begin{figure}
\centering
\input{blockdiagram.tex}
\caption{A description of the implementation of the floating point $\log_2$ which MetiTarski was used to verify. The input is a 32 bit floating point number where $s$ is the sign bit, $e$ is the exponent, $y$ is the top 6 bits of the significand and $w$ ($\overline{w} = !w$), $X$ and $Z$ are divisions of the significand. The last bit is discarded.\label{block_diagram}}
\end{figure}

% Description of implementation
Figure \ref{block_diagram} gives a description of the floating point implementation verified using this methodology. This implementation was a release candidate for Cadence Design Systems. We see that the 23 bit significand is split into 4 used sections, whilst the last bit is discarded. The top 6 bits, $y$, are passed to the LUT which outputs 3 polynomial coefficients. Using these coefficients, the algorithm computes a polynomial approximation to the logarithm. The split significand is used to reduce the number of operations along with a truncation of bits which are insignificant for the accuracy target. The accuracy target for the implementation was 4 ULPs (unit in last place) \cite{muller2010handbook}, which is why, for the following conjectures the bounds on the distance from the true logarithm are $2^{-21}(=4\times 2^{-23})$, since we consider inputs in the region [1,2). The exponent, $e$, in our input region is equal to the IEEE-754 bias $b$, yielding a zero exponent in the decoded value. This was the first implementation verified using the MetiTarski method, and as a result, when this was investigated the floor function was not supported. However, rather than calling the floor function explicitly the bounds on the calculations were calculated by hand and combined to yield a simple bound on the error in either direction. This is a labour intensive method, and was the inspiration behind the introduction of the floor function. This manual approach, resulted in two separate template problems. 

% Outline template problems
\begin{align*}
    \textrm{Template 1} &\Rightarrow approx \leq exact + 2^{-21} \\
    \textrm{Template 2} &\Rightarrow approx \geq exact - 2^{-21}
\end{align*}
\begin{multline*}
(\textrm{ }l=0.69314718055994530941723212145817\textrm{ } \&\textrm{ } 0\leq X \leq2^9-1\textrm{ } \& \textrm{ }0\leq Z \leq 2^6-1\textrm{ }) \Rightarrow \\ 
\textbf{Template 1} \\
  \textrm{ }  2^{-33} a\Big(2^9(1-w)+2X(w-1)+ 2^{-9}X^{2}\Big)   +  2^{-38} b\Big(Z +2^{6}X + 2^{15}(w-1)\Big)+  2^{-23}c\\
-  \frac{1}{l}\ln(1+y+2^{-7}w+2^{-16}X+2^{-22}Z)      \leq 2^{-21}+2^{-33}a 
\end{multline*} 
\begin{multline*}
\textbf{Template 2} \\
2^{-33} a\Big(2^9(1-w)+2X(w-1)+ 2^{-9}X^{2}\Big)   +  2^{-38} b\Big(Z +2^{6}X + 2^{15}(w-1)\Big)+  2^{-23}c\\
-  \frac{1}{l}\ln(1+y+2^{-7}w+2^{-16}X+2^{-22}Z)      \geq -2^{-21}+2^{-23}
\end{multline*}

% Describe the templates
Upper case letters are variables with defined ranges, which correspond to the different sections of the significand described in Figure \ref{block_diagram}. The lower case letters are constants in each problem, which are replaced by the python wrapper script, where in particular $w\in \{0,1\}$, is a single bit of the significand as shown in the diagram. We note here that the splitting of the significand has forced the use of two variables, $X$ and $Z$, to model the problem. On the right hand side of the inequality in Template 1, the $2^{-33}a$ is an error term introduced to model the truncation of the squared term in the algorithm. More precisely, we are explicitly using the naive floor function bound,
\begin{equation} \label{a_trunc_term}
    \left \lfloor2^9(\overline{w}-2^{-9}X)^2\right \rfloor  
    \geq 
    (2^9(\overline{w}-2^{-9}X)^2-1).
\end{equation}
In Template 1 we are trying to prove an upper bound on the approximation. As $a$ is negative we want to make the square term as small as possible. This should not be surprising since the coefficient of the square term in the Taylor expansion of $log_2(1+x)$ is negative. We move the error term to the right hand side of the inequality for readability. 

In these conjectures, $l$, is a high precision approximation to $\ln(2)$, which is necessary because MetiTarski only supports the natural logarithm, meaning we need to switch the base. Generated using MPFR \cite{fousse2007mpfr}, it was truncated at the 32nd decimal place. Therefore $0<\ln{(2)}-l<10^{-32}$. This error can be accounted for by the over-approximation of the truncation error term described by equation \ref{a_trunc_term}. This lower bound can actually be made tighter as we only truncate off at most 9 bits, so becomes
\begin{equation*} 
    \left \lfloor2^9(\overline{w}-2^{-9}X)^2\right \rfloor  
    \geq 
    (2^9(\overline{w}-2^{-9}X)^2-(1-2^{-9})).
\end{equation*}

This means that we have over-approximated the error by $2^{-42}a$. The error introduced by using $l$ is
\begin{equation*}
    \frac{1}{l}\ln{v} - \frac{1}{\ln{2}}\ln{v} = 
    \frac{\ln{2}-l}{l\times \ln{2}}\ln{v} < \frac{10^{-32}}{l \times \ln{2}} \ln{v}. 
\end{equation*}
For the cases we are interested in $\abs{\ln{v}}\leq 2$ and $a\geq 10^{-1}$ in all the polynomials. This means that the error introduced by the $l$ approximation is less than $2^{-42}a$ and is accounted for in Template 1. 

% Other half of template
Of course, Template 1 only proves half of the verification problem. We must also prove the other side, Template $2$, which we can AND with the first, allowing us to keep the same coefficients and variable bounds. The only difference is that we check the lower bound and also calculate a new error bound. In this case, the $2^{-23}$ term on the right hand side of the inequality is to account for the final truncation and rounding of the output in order to fit it into floating point format. It relies on the same floor function bound used in equation \ref{a_trunc_term}, but applied to the whole polynomial rather than a single term. It has the opposite sign to the previous error term and is moved to the other side of the inequality once again. There is no need to account for the error in $l$ in this template as $\ln{(2)}-l>0$. If we can satisfy both these conjectures then we have obtained a proof for the given input range, in our case [1,2). 

% Proofs and number of problems (plus error variable?)
An error variable, $\epsilon \in [2^{-33}a, 2^{-23}]$, could have been used to reduce this to just one template problem, however testing showed that it was more efficient to split the conjecture in two to avoid the additional variable. The proofs of these problems and minor variants for inputs in the region [0.5,1) were obtained successfully with runtimes presented in Table \ref{result}. With a LUT containing 32 entries, the total number of MetiTarski problems for this verification task was 128 (= 32 $\times$ 2 $\times$ 2), since each entry gave two problems and we needed slightly modified templates for the [0.5,1) region. 

% Hand proof and breach finding
Previously, the implementation had been checked using a basic benchmarking tool. Through our investigations, we discovered that the expected accuracy of the implementation was breached for a small input region. Since exponent scaling for $\log_2$ simply involves adding the exponent to the approximation made on the $1.\textit{significand}$ component, a bespoke and simple proof was sufficient to complete the verification. However, the proof identified a region in which the claimed accuracy may be breached and exhaustive testing on this region identified a large number of counterexamples. For the purpose of validating our methodology, we exhaustively tested all inputs: as predicted, everywhere but in this narrow region the implementation met its accuracy claim.

% Second implementation
The second implementation in Table \ref{result} is an experimental test case generated primarily to analyse how the MetiTarski method performed on higher precision algorithms. It takes as input a fixed point number with 8 bits before the decimal place and 24 bits after. Therefore, the total number of possible inputs is $2^{32}$, but it is a higher precision algorithm and is more accurate as it makes use of a larger lookup table (256 entries). The algorithm used in this implementation is very similar to the one presented in Figure \ref{block_diagram}, taking a fixed point input, therefore we shall not discuss the details of the implementation. The template problem files were also extremely similar, if not simpler, but the main focus of this analysis was to test how the technique preformed on higher precision algorithms, as the claimed accuracy of this implementation was $2^{-28}$. We found that the technique was equally successful in verifying this implementation. 

% Table of results
\begin{table}
\begin{tabular}{lll}\toprule
Design & Floating point $\log_2$ & Fixed point 8.24 $\log_2$    \\
\midrule
 Input Region Verified& $[0.5,2)$ & $[1,2) $ \\ 
 MetiTarski Problems & 128 & 512 \\
 MetiTarski Time (mins.)$^a$ &  7 & 4 \\ 
 Exhaustive Test Time (mins.)$^b$ & 42 & 53 \\
\bottomrule
\end{tabular}
\caption{Verification runtime results for industrial implementations of logarithm base 2. The floating point implementation is a binary 32 bit IEEE-754 implementation.\newline
a. Run on Intel Xeon E5 2698, speed: 2.3 GHz, CPUS: 2, cores: 2, cache-size: 40MB
\newline
b. Run on Intel Xeon X5680 machines, speed: 3.33 GHz, CPUS: 2, cores: 12, cache-size: 12MB}\label{result}     
\end{table}

% Speedup
We see that the speedup achieved by MetiTarski is significant on the floating point logarithm. Notably it was even greater for the higher precision (8.24 fixed point) experimental implementation. Such an observation suggests that this methodology could be viable for verification of higher precision hardware, where traditional exhaustive techniques are infeasible. Evidence supporting this claim will be given in the \S \ref{flopoco}.

% Complex algo, not fully verified, limitations
In addition to the results above, we applied our technique to a more complex algorithm, which had already been formally verified. Here we were only able to prove a subset of the problems. If we were attempting to verify a new implementation this outcome is still helpful as we reduce the space of inputs on which exhaustive testing is necessary. Across our experiments this was the only case where we found it necessary to resort to exhaustive testing. Through this experience we discovered a significant shortcoming of the approach. Generating the initial template problem is a non-trivial task. The verification engineer needs a deep understanding of the design implementation in order to model the errors correctly. Floor functions may be used initially to establish whether the algorithm is correct to a higher error rate, but to obtain a full proof, several refinements of the model may be required. Such refinements should take into account correlations in the error terms for the different sub-intervals. This aspect of the technique becomes time consuming when trying to verify more involved algorithms. For example, a sequence of \textbf{if} and \textbf{else if} statements, which can be found in some hardware algorithms, and much more commonly in software, are difficult to model using this technique. By comparison, exhaustive testing is far simpler to implement: it requires only knowledge of the output and the correct answer, which we may reasonably assume we have given the existence of the MPFR library \cite{fousse2007mpfr}. This fact currently limits the circumstances under which our method is useful. For example, iterative algorithms for division and square root which use a variable number of iterations depending on the input are likely intractable. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Automatically Generated Designs: FloPoCo
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Automatically Generated Designs: FloPoCo}
\label{flopoco}
% Intro to Flopoco
An additional source of example implementations we can verify can be generated by the FloPoCo tool \cite{DinechinPasca2011-DaT}. FloPoCo can generate arithmetic cores for a range of applications, with its primary focus being on FPGAs. Its operators are fully parameterisable in precision and can be given error specifications to meet. The tool outputs synthesisable VHDL, that is generally human readable. It is still actively developed and used by the academic community, winning the \textit{Community Award} of FPL 2017. FloPoCo has a wide range of methods and operators that it can generate, but we will focus on just one of these.

% FixFunctionByPieceWisePoly option
The \textit{FixFunctionByPiecewisePoly} option generates a piecewise polynomial VHDL implementations, for a given function $f(x)$, where $x\in [0,1]$ and has a user specified number of bits. The user specifies the degree of the polynomial $n$, which will be used in the approximation. By default, it generates an approximation to $f(x)$ that it claims is correct to within 1 ULP. The underlying method to generate these approximations is described in \cite{DinJolPas2010-poly}. 

% Polynomial approx
Using even intervals they generate polynomial coefficients for each interval, where the most significant bits of the input $x$, determine which polynomial is used. This is implemented as a LUT with one entry per polynomial/interval and width determined by the combined width of the coefficients. One of the benefits of using FloPoCo is that the user does not need to specify the number of intervals to split the domain into. The tool finds a number of intervals for which it can find a set of polynomials of degree $n$, that meet the error specification allowing some of the error budget for rounding errors in implementing the polynomial evaluation. They do not claim that this is the smallest number of polynomials that would achieve the error budget. 

% Modified Remez Algo
Generating the polynomial coefficients is done using a modified Remez algorithm \cite{brisebarre2007efficient}, that essentially computes minmax polynomials over the space of polynomials with coefficients with finite precision. This removes the need to round real coefficients, avoiding additional error. The Sollya tool \cite{ChevillardJoldesLauter2010} handles the polynomial generation and provides an error bound. 

% Polynomial Evaluation
Polynomial evaluation is then implemented using the Horner evaluation scheme, 
\begin{align*}
    p(x) &= a_n x^x + a_{n-1}x^{n-1} + ... + a_0 \\
         &= (...(a_n x + a_{n-1})x + ...+a_1)x + a_0.
\end{align*}
Internal truncation is used in this implementation because $x\in [0,1]$, implies that there is no need to compute $a_n x^n$ to full precision since most of its bits will not be significant. The architecture of the implementation is described by Figure \ref{horner_arch} which motivates the error modelling in our MetiTarski problems.

\begin{figure}
\centering
\input{horner_arch}
\caption{Architecture of the \textit{FixFunctionByPiecewisePolynomial} option generated by FloPoCo \cite{DinJolPas2010-poly}. The T-FMA units are truncated fused-multiply adds, implementing $(\alpha\times \beta + \gamma)$. The Trunc operators just truncate the bits of $z$ as not all of them are required for the FMA operation.\label{horner_arch}}
\end{figure}

We will verify the 23 bit and 52 bit examples given in \cite{DinJolPas2010-poly}, which use degree 2 and degree 4 polynomials respectively to approximate $\sqrt{1+x}$. Table \ref{flopo_results} shows some details of the implementations and the verification effort required. 

\begin{center}
\begin{table}
\begin{tabular}{lllllll}
\toprule
Bits & Polynomial Degree & LUT Entries & Problems & Variables & Total Time (min) & Average Time (sec)\\
\midrule
23 & 2 & 64 & 128 & 2 & 2.4 & 1.13 \\
52 & 4 & 256 & 1024 & 3 & 180 & 10.8\\
\bottomrule
\end{tabular}	
\caption{Verification results for FloPoCo generated square root implementations.}\label{flopo_results}     
\end{table}
\end{center}

To illustrate the method applied to this example, consider just the 52 bit case. The LUT contained 256 entries corresponding to 256 different polynomials. We extract the coefficients table from the VHDL and use a wrapper script to insert these values as well as the top bits of the input into the template problem, as described in the general methodology. The implementation uses 4 truncated multiply add blocks. The truncation of the remaining input bits $z$ for each of these, as shown in Figure \ref{horner_arch}, forces us to split $z$ into 4 MetiTarski variables:
\begin{center}
\begin{tikzpicture}
%draw input nodes
\node [minimum width = 2cm,minimum height=0.5cm] at (0,0) (z) {$z=$};
\node[rectangle, draw, minimum width = 3cm, minimum height = 0.5cm] at (2,0) (z1) {$z_1$};
\node[rectangle, draw, minimum width = 2cm, minimum height = 0.5cm] at (4.5,0) (z2) {$z_2$};
\node[rectangle, draw, minimum width = 1cm, minimum height = 0.5cm] at (6,0) (z3) {$z_3$};
\node[rectangle, draw, minimum width = 0.5cm, minimum height = 0.5cm] at (6.77,0) (z4) {$z_4$};
\end{tikzpicture}
\end{center}

\begin{equation*}
    z_1 \in [-2^{-9}, 2^{-9}-2^{-27}], \,
    z_2 \in [0, 2^{-27} - 2^{-36}],\,
    z_3 \in [0, 2^{-36} - 2^{-46}],\,
    z_4 \in [0, 2^{-46} - 2^{-52}]
\end{equation*}

To centre the interval in the VHDL implementation the $z_1$ variable is treated as signed. In addition to the truncations on the input bits to the FMAs they are also truncated on the output, which we model as errors $E_i$ for each FMA. With an error specification that says we should be within 1 ULP of the exact square root, this yields an initial MetiTarski problem of the form:

\begin{align*}
    |\sqrt{1+y+z_1+z_2+z_3+z_4} - ((A_4 * z_1 + A_3 - E_4)&*(z_1+z_2)\\
                                  + A_2 - E_3)&*(z_1+z_2+z_3)\\
                                  + A_1 - E_2)&*(z_1+z_2+z_3+z_4)\\
                                  + A_0 - E_1)&- E_0 | < 2^{-52}
\end{align*}

Maximising the total error, and splitting based on $z_1$ positive or negative, splits this initial template into into 4 problems.
Unfortunately, MetiTarski was unable to handle this 4 variable problem, which is at the limit of how many variables MetiTarski can handle. 

We solved this problem by only using 3 variables, essentially introducing $z_3' = z_3+z_4$. Then wherever we want to use just $z_3$, we can minimise/maximise the value of $z_4$ and use the bounds $z_3' - (2^{-46} - 2^{-52}) \leq z_3 \leq z_3'$. Splitting the problems based on $z_1$ positive and negative and minimising/maximising the error in the polynomial evaluation yields 4 problems per interval/polynomial. Since our output has 52 bits after the decimal place, 1 ULP corresponds to a total error bound of $2^{-52}$. The 23 bit example uses a similar modelling approach but only needs two problems per interval/polynomial. 

The results are summarised in Table \ref{flopo_results}, where we see that each 53 bit problem takes 10x longer to solve on average. This is due to the extra variable and the tighter error bound as well as the increased complexity due to the higher polynomial degree. A 3 hour verification runtime is certainly reasonable for a binary64 implementation, which clearly cannot be exhaustively tested.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% A comparison with Gappa
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{A Comparison with Gappa}
\label{gappa}

% Gappa: A comparison
Having given several examples of how our approach can be used, we shall now look at a comparison with an existing tool. For verification of small floating point functions, the Gappa tool \cite{de2006assisted,boldo2009combining} has been tested and is still being developed. Gappa uses interval arithmetic in order to prove conjectures whilst MetiTarski relies on cylindrical algebraic decomposition to generate proofs. Both methods are valid; interval arithmetic is weaker but considerably more efficient. The approach using the Gappa tool, is the most comparable method to that described in this paper, therefore we reproduced the verification of the polynomial approximation to the sine function implemented as part of the CRlibm project \cite{daramy2009cr,5483294}. The function verified here is a binary64 software implementation using \textit{round to nearest}, which demonstrates not only the versatility of MetiTarski, but also shows that the axioms bounding the sine function are sufficiently accurate to verify a 64 bit implementations.

The implementation verified here does not use lookup tables, so is a simpler case than we have been considering in the previous sections. In essence we can think of this as just using a single table entry to yield the polynomial coefficients for all inputs, rather than sub-dividing the input domain. As a result the verification method deployed here uses a simpler approach than that described by Figure \ref{procedure_flow}, eliminating the need for template problems, wrapper scripts or hundreds of MetiTarski calls. 

The following C code quoted from \cite{5483294}, is compiled respecting the C99 and IEEE-754 standards, with all variables being binary64. 

% C-code:
\begin{lstlisting}[language=C]
s3 = -1.6666666666666665741e-01; // approximately -1/6
s5 =  8.3333333333333332177e-03; // approximately  1/120
s7 = -1.9841269841269841253e-04; // approximately -1/5040

yh2 = yh * yh;
ts  = yh2 * (s3 + yh2 * (s5 +yh2 * s7));
Fast2Sum(sh, sl, yh, yl + yh * ts);
\end{lstlisting}

In this code, the input is represented as a sum of two binary64 arguments, y = yh+yl, where yh represents the most significant part and yl the least significant, and the result, s, is similarly represented as a sum sh+sl. The Fast2Sum algorithm provides an exact representation of the sum of two floating point numbers as a pair of floating point numbers, namely no rounding errors are incurred. This approximation to sine is only valid for a limited input region, namely values of magnitude less than $6.3\times 10^{-3}$. The code is an approximation to the Taylor expansion of sine, where the terms involving the least significant bits are discarded since they are sufficiently small. 
\begin{equation}
    \sin{(y)} = y - \frac{y^3}{6} + \frac{y^5}{120} - \frac{y^7}{5040} + ... 
\end{equation}

The Gappa approach makes use of the ``IEEEdouble'' shortcut for the IEEE-compliant binary64 \textit{round to nearest} mode, in order to encode the rounding errors in this implementation. Since the IEEE-754 standard imposes exact arithmetic in binary64 \textit{round to nearest} mode, the maximum relative error incurred in each arithmetic operation is $2^{-53}$, since binary64 uses a 52 bit significand. This keeps the error modelling for our MetiTarski problems far simpler than in some hardware algorithms, which can operate at the bit level. Essentially, for each of the operations in the C code, a maximum relative error of $2^{-53}$ can be incurred. This property is commonly described as the $(1+\epsilon)$ lemma. Harrison describes the bounds on $\epsilon$ in \cite{harrison2006floating}.

The full Gappa script is given in \cite{5483294}, and for the purpose of comparison we use the same outline of the problem to prove, just using more explicit error modelling. They also introduce the bound on the input variable $|Y|\geq 2^{-200}$, which is necessary to make sure that the relative errors due to underflow remain bounded. In the Gappa script, 4 variables are used, since the sine is represented by a variable $S\in[0,1]$. MetiTarski supports axiomatic bounds on functions such as sine, so this variable is dropped in order to reduce the problem to 3 variables. As Gappa has no notion of sine, first the approximation error between the exact polynomial approximation and sine had to be calculated. This approximation error takes no account of any floating point rouding errors. For this purpose the Sollya tool \cite{ChevillardJoldesLauter2010}, was used to prove that,
\begin{equation}
|\mathrm{Y}| \leq 6.3 \times 10^{-3} \Rightarrow\left|\frac{\mathrm{PolySinY}-\operatorname{SinY}}{\operatorname{SinY}}\right| \leq 3.7 \times 10^{-24}.
\end{equation}
PolySinY represents the exact polynomial specified in the C code above with no floating point rounding errors. In MetiTarski, no additional tools were required, as the inbuilt axiomatic bounds on the sine function were sufficient. The variable $Y$, is the sum of our two binary64 variables $H$ (msb) and $L$ (lsb), with a bound on the relative error in the argument reduction stage, also present in the Gappa paper.
\begin{equation*}
    \left|\frac{H+L-Y}{Y}\right| < 2.53\times 10^{-23} \quad \land \quad |L|<2^{-53}|H|.
\end{equation*}

The goal is to prove that the maximum relative error in our approximation of $\sin{(Y)}$, is less than $2^{-67}$. Since we explicitly calculate the error modelling, and have used placeholder replacement as a strategy throughout the previous sections, the template problems include error placeholders $e_i$ for $i=0,...,7$, where each $e_i \in [1-2^{-53},1+2^{-53}]$, representing the relative error in each arithmetic operation. With these errors inserted the polynomial approximation to sine after expanding out all the brackets reduces to
\begin{align*}
    f(H,L, e_0,..., e_7) &= H + e_7(L + g(H,L, e_0,..., e_6)).\\
    g(H, e_0,..., e_6) &= e_0e_4e_5e_6\times aH^3 + e_0^2e_2e_3e_4e_5e_6\times bH^5 + e_0^3e_1e_2e_3e_4e_5e_6\times cH^7
\end{align*}
Ideally we would introduce a new MetiTarski variable for each $e_i$, however MetiTarski is unlikely to terminate when using more than 4 or 5 variables, for the reasons described in \cite{akbarpour2008metitarski}, so we need to make choices to bound these errors. By considering only $H>0$, and choosing appropriate values for $e_i=1\pm 2^{-53}$, we obtain 
\begin{equation} \label{bound_fun}
g_{\textrm{min}}(H) \leq g(H,e_0,...,e_6) \leq g_{\textrm{max}}(H) \; \forall e_i \in [1-2^{-53},1+2^{-53}].
\end{equation}
We consider only $H>0$, which implies that $\sin{(Y)}>0$. We combine this with the constraints described above. 
\begin{align}
    &\left|\frac{f(H,L, e_0,...,e_7)-\sin{(Y)}}{\sin{(Y)}}\right|<2^{-67} \quad \forall e_i \in [1-2^{-53},1+2^{-53}] \label{orig_prob} \quad \Leftrightarrow \\
    & f(H,L,e_0,...,e_7) < (1+2^{-67}) \sin{(Y)} \, \land \, f(H,L,e_0,...,e_7) > (1-2^{-67}) \sin{(Y)} 
    \\
    & \forall e_i \in [1-2^{-53},1+2^{-53}].
\end{align}
Using our bounding functions (eqn \ref{bound_fun}), we can reduce the problem to proving the four inequalities
\begin{align}
    &H + e_7(L+g_{\textrm{max}}(H)) < (1+2^{-67}) \sin{(Y)}\label{upper_bound} \\
    &H + e_7(L+g_{\textrm{min}}(H)) > (1-2^{-67}) \sin{(Y)}\label{lower_bound} \quad \textrm{for}\,  e_7=1\pm2^{-53}.
\end{align}
Due to the error bounding approach and the anti-symmetry of sine and the polynomial in $H$, proving these four inequalities is sufficient to verify the original problem (eqn. \ref{orig_prob}).

MetiTarski required some minor assistance with a basic rewrite for one of the inequalities, just replacing the variable $L$ with its relevant bound. The full proof scripts are given in Appendix \ref{sine_appendix}. The 4 problems were provable using MetiTarski, with a combined runtime of 460 seconds on an Intel I7-3517U CPU. 

We will briefly digress, as these experiments also highlighted some interesting MetiTarski behaviour. MetiTarski invokes an RCF solver during its operation, and it can be configured to use any one of Z3 \cite{de2008z3}, QEPCAD B \cite{brown2003qepcad} or Mathematica (invoked in batch mode) \cite{Mathematica}. Given that Mathematica and Z3 are widely used in academia and industry we choose to trust their results. Only Mathematica terminates in a reasonable amount of time in this problem domain. In fact, using Z3 or QEPCAD B, MetiTarski hangs when trying to prove the significantly easier problem,
\begin{equation}
    H + (1-2^{-53})\times(L+g(H,0,...,0)) < (1+2^{-67})\sin{(Y)}.
\end{equation}
This introduces no errors in our function $g$. This discovery suggests that there could be further development in this field. 

To conclude this section, we should compare the Gappa method to ours. Gappa's understanding of floating point rounding via the shortcut described above makes it simpler to generate problem scripts when analysing \textit{pure} floating point algorithms. By \textit{pure} we mean that the algorithm only uses standard floating point arithmetic with no bit level manipulations. These are common in software, such as the CRlibm sine function, but in hardware, designers rarely stick to \textit{pure} floating point arithmetic, which would likely pose a different challenge for generating Gappa scripts. The Gappa approach had to involve other tools in its solution, whilst MetiTarski could be used in a standalone manner. With Gappa taking less than a second to generate a proof its clearly demonstrates stronger performance. However, Gappa required 6 additional ``hints'', with the authors describing the hint writing process as the most time consuming part. By comparison, MetiTarski required just one re-write of a problem in order to prove it, and no re-writes (or ``hints'') were required in our other examples, so MetiTarski is competing on an uneven playing field. In both approaches it is evident that writing the respective scripts is the time-consuming component rather than generating the proofs.

Given the availability of tools with inbuilt floating point rounding, such as Gappa, most software implementations will be more amenable to these tools rather than MetiTarski. However, this comparison has shown that the process of obtaining a proof involving an implementation of sine, using MetiTarski, is similar to the approach described for the logarithm earlier. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Related Work
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related work}
\label{Related}
% General floating point challenges
An important problem for verification will be the interplay of floating point and bit level operations, common in real code. As observed by de Dinechin, Lauter and Melquiond, expert developers will arrange code in order to take advantage of situations where floating point arithmetic is exact, such as multiplication by a power of two, to name just one \cite{de2006assisted}. Another trick, computing $2^n$ in double precision, can be done by shifting $n+1023$ left by 52 bits \cite{mine2012abstract}. Such techniques make verification more challenging, particularly in the context of high precision transcendental functions \cite{lee2016verifying}. Code obfuscation can easily occur in the optimisation stages, when the underlying mathematical approximations become obscured as expressions are manipulated and re-ordered in order to maximise floating point accuracy.

% Harrison & HOL Light
We have already described the Gappa approach, but many other tools have been applied to floating point algorithm verification. John Harrison conducted some of the earliest and most rigorous work via interactive theorem proving, using the HOL Light proof assistant~\cite{harrison1997floating}. Above, we highlighted how Harrison had developed an entire theory of floating point arithmetic \cite{harrison1999machine}. He defined the set of representable floating point numbers as a parameterised triple $(E,p,N)$, which is the set of real numbers representable in the form $(-1)^s 2^{e-N} k$ with $e<E$, $k<2^p$ and $s<2$. Formalizations of the four standard rounding modes (nearest, down, up, zero), the widely used $(1+\epsilon)$ lemma and other lemmas about exactness are constructed in HOL Light. As a result of this underlying theory, most floating point problems are ignored and he is able to reason about real numbers and straightforward algebraic calculations \cite{harrison1999machine}. Such an approach is labour intensive and not easily adaptable to changes in the algorithm: for example an increase in the input bit width would require a large portion of the work to be done again. Note however, that Harrison's proofs are more detailed and refer to specific features of the IEEE floating point standard, such as the various available rounding modes. 
%Additional comments about Z thm prover.
% Later Harrison developed libraries for the reasoning of floating point numbers in HOL Light \cite{harrison1999machine,harrison2000formal}. Before any of this Barrett applied formal methods to describe the IEEE-754 and -854 standards for Z \cite{barrett1989formal}.

% CoqInteval and FPTaylor
 The Coq system has also been the subject of several attempts to develop libraries \cite{daumas2001generic,melquiond2012floating,boldo2011flocq} and was combined with Gappa \cite{boldo2009combining,daumas2010certification} in order to verify a wider range of floating point algorithms. As well as these, Boldo combined Coq with Caduceus for the verification of floating point C programs \cite{boldo2007formal}.
 
 CoqInterval \cite{martin2016proving} targets a very similar goal to the one presented in this paper. It is primarily concerned with verifying floating point mathematical libraries and as a result many of the problems look similar to ours, although there is less focus on piece-wise polynomial approximations. As a result CoqInterval is likely capable of proving many of the problems presented here. The key distinction is that CoqInterval is based on interval arithmetic, which is generally weaker than cylindrical algebraic decomposition (MetiTarski's underlying technology). The CoqInterval authors actually compared CoqInterval to MetiTarski, noting that MetiTarski was generally faster and could tackle more complex problems \cite{martin2016proving}. However CoqInterval uses automated Taylor expansion, whilst MetiTarski relies on its built in axioms so is more restricted. Also CoqIntervals proofs are formally verified by Coq which may account for some of the performance difference. 
 
% Other Provers
FPTaylor is another recent tool, that bounds floating point round off errors again using Taylor expansion \cite{solovyev2018rigorous}. FPTaylor's benchmarks show that it is capable of tackling problems with up to 6 variables, with support for elementary functions. In addition to these studies several theorem provers have been applied to floating point verification problems, but in the cases referenced here additional libraries were developed to handle floating point numbers. The ACL2 prover was applied to floating point division, multiplication and square root algorithms \cite{russinoff1998mechanically,moore1996mechanically}. Lastly the PVS prover was used to verify the floating point unit of a complete microprocessor \cite{jacobi2005formal}. In general, the main difference between this paper and those referenced above is that we target higher level algorithms for verification and don't attempt to encode floating point operations in MetiTarski explicitly. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Conclusion
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion}
\label{Conclusion}
We have successfully verified a variety of fixed and floating point logarithm, sine and square root implementations using the MetiTarski theorem prover.
We have also identified a bug in an algorithm that was close to production. Most interesting are the binary64 results, which suggest that this technique will be viable when higher precision algorithms are required. In a comparison with a more thoroughly explored technique, using Gappa, our approach held its own in successfully verifying a binary64 software implementation of the sine function. This highlights the potentially broad scope of this approach, although more complex software algorithms may not be amenable. 

For a set of algorithms following a basic outline, it is possible to automate the whole verification procedure after the initial template and wrapper scripts are written. In this type of setting, the technique is quite powerful. However, for algorithms not following a standard format, the verification process requires significantly more manual effort by a skilled verification engineer than alternative methods. For some cases, the approach may be unsuccessful if the algorithm uses techniques we can't model using MetiTarski. In all these examples the bulk of the time was spent manually error modelling with the MetiTarski proof times being relatively short, at most a few hours. 

This first investigation into this technique has demonstrated both strengths and weaknesses, but further research may yield new methods for modelling more complex algorithms. Further development of MetiTarski could enhance its power or simplify the process, as the introduction of the floor function has already done. Continuing MetiTarski development to add new function bounds, such as for the base 2 logarithm would make future work simpler. A possible extension is the use of a binary chop applied to the input region of each problem, which may allow the engineer to further reduce the input space on which exhaustive testing is necessary. In this work there has generally been no verification of the argument reduction stage described in \S \ref{trans functs}, so this may be an interesting topic to explore in the future. For implementations that follow a standard model and particularly bespoke higher precision implementations, the methodology shows promise.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Acknowledgements
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Acknowledgements}
We would like to thank Mathew Eaton and Ran Zmigrod for helpful contributions during the initial investigations. Much of the preliminary work for this paper was done at Cadence Design Systems, who were supportive throughout. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Bibliography
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibliographystyle{alpha}   
\bibliography{bibliography.bib}  

\appendix
\section{Sine Approximation Problem} \label{sine_appendix}
We show the MetiTarski problems from \S \ref{gappa}, which uses 3 variables and the sin-extended axioms. There are 4 conjectures, which are subject to the same constraints. Variables are represented by capital letters and are declared within the square brackets. The constants are the result of compiling the C code from \S \ref{gappa}, adhering to the C99 standard. We shall present one script in full, then the other 3 just need to have the inequality to prove replaced. 
\begin{itemize}
    \item Y - the argument of sine
    \item H - the msb of Y (approx)
    \item L - the lsb of Y (approx)
\end{itemize}

\begin{lstlisting}
fof(crlibm_sin, conjecture, ! [Y,H,L] : 
((
a = -0.1666666666666666574148081281236954964697360992431640625          &
b =  0.00833333333333333321768510160154619370587170124053955078125      &
c = -0.0001984126984126984125263171154784913596813566982746124267578125 &
Y                           : (=2^(-200), 6.3e-3=)                      &  
H                           > 0                                         &
L                           : (=-2^(-53)*H, 2^(-53)*H=)                 &
H+L-Y                       : (-Y*2.53e-23, Y*2.53e-23)                 
) 
=>
( 
H + (L + ((1-2^(-53))^4 * a * H^3) 
+ ((1-2^(-53))^5 * (1+2^(-53))^2 * b * H^5) 
+ ((1-2^(-53))^7 * (1+2^(-53))^2 * c * H^7))*(1+2^(-53)) 
< (1+2^(-67))*sin(Y)    
))).

include('Axioms/general.ax').
include('Axioms/sin-extended.ax').
\end{lstlisting}


\noindent\textbf{Additional inequalities to prove:}
\begin{lstlisting}
H + (L + ((1-2^(-53))^4 * a * H^3) 
+ ((1-2^(-53))^5 * (1+2^(-53))^2 * b * H^5) 
+ ((1-2^(-53))^7 * (1+2^(-53))^2 * c * H^7))*(1-2^(-53)) 
< (1+2^(-67))*sin(Y)    

H + (L + ((1+2^(-53))^4 * a * H^3) 
+ ((1+2^(-53))^5 * (1-2^(-53))^2 * b * H^5) 
+ ((1+2^(-53))^7 * (1-2^(-53))^2 * c * H^7))*(1+2^(-53)) 
> (1-2^(-67))*sin(Y) 

H + L + (((1+2^(-53))^4 * a * H^3) 
+ ((1+2^(-53))^5 * (1-2^(-53))^2 * b * H^5) 
+ ((1+2^(-53))^7 * (1-2^(-53))^2 * c * H^7))*(1-2^(-53)) - 2^(-107)*H 
> (1-2^(-67))*sin(Y) 
\end{lstlisting}
\end{document}

